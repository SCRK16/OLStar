{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OL* vs L*\n",
    "\n",
    "We will compare the L* algorithm by Angluin (1987) with our new OL* algorithm, which decomposes a Mealy machine into components, where each components models the behaviour of the SUL for a single output.\n",
    "\n",
    "We ran both OL* and L* on the benchmarks from Labbaf et al. (2023). This benchmark consists of the parallel interleaving of multiple smaller automata. Note that the components of these machines can be split by their input alphabet, but not necessarily by their output alphabet. Some outputs are unique for a component, and can thus by learned efficiently by OL*, while others appear in multiple components (like the output **1**), so these will be tricky for OL* to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model                      19418\n",
       "Stages                       212\n",
       "States                    148781\n",
       "Short rows                111403\n",
       "Inconsistent count          5068\n",
       "Zero outputs count             0\n",
       "Two outputs count              0\n",
       "Learning queries       164259359\n",
       "Learning symbols      2157248568\n",
       "Testing queries         68686443\n",
       "Testing symbols        746068858\n",
       "dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "data_olstar = pd.read_csv(\"D:\\\\Data\\\\results_labbaf_olstar.csv\")\n",
    "data_lstar = pd.read_csv(\"D:\\\\Data\\\\results_labbaf_lstar.csv\")\n",
    "\n",
    "data_olstar.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: After making the table consistent, there were no instances were the resulting machine returns zero or multiple outputs at the same time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_olstar[data_olstar[\"Testing queries\"] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_lstar[data_lstar[\"Testing queries\"] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: All testing queries for OL* come from only 8 models. In contrast, L* needs testing queries for 37 models.\n",
    "\n",
    "We will first analyse the performance of OL* against the performance of L*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL* Learning: 838_057.95\n",
      "L* Learning:  421_086.48\n"
     ]
    }
   ],
   "source": [
    "print(\"OL* Learning:\", f\"{data_olstar[\"Learning queries\"].mean():_.2f}\")\n",
    "print(\"L* Learning: \", f\"{data_lstar[\"Learning queries\"].mean():_.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL* Testing: 1_675_059.39\n",
      "L* Testing:    350_441.04\n"
     ]
    }
   ],
   "source": [
    "print(\"OL* Testing:\", f\"{data_lstar[\"Testing queries\"].mean():_.2f}\")\n",
    "print(\"L* Testing:   \", f\"{data_olstar[\"Testing queries\"].mean():_.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL* Total: 1_188_498.99\n",
      "L* Total:  2_096_145.87\n"
     ]
    }
   ],
   "source": [
    "def total_queries(df):\n",
    "    return (df[\"Learning queries\"] + df[\"Testing queries\"]).mean()\n",
    "\n",
    "print(\"OL* Total:\", f\"{total_queries(data_olstar).mean():_.2f}\")\n",
    "print(\"L* Total: \", f\"{total_queries(data_lstar).mean():_.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at just the number of learning queries, L* outperforms OL*. However, OL* more than makes up for this with the number of testing queries needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing queries\n",
    "\n",
    "Many models do not require any equivalence queries to learn correctly (ignoring the last equivalence query which returns \"true\"). From initial testing, OL* seemed to have a big advantage for models that _do_ require equivalence queries. It requires fewer equivalence queries, which results in fewer testing queries. We will now analyse how big the advantage of OL* is for these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "4_491_453.76\n",
      "9_602_850.46\n"
     ]
    }
   ],
   "source": [
    "tq_olstar = data_olstar[data_lstar[\"Testing queries\"] > 0]\n",
    "tq_lstar = data_lstar[data_lstar[\"Testing queries\"] > 0]\n",
    "\n",
    "print(f\"{total_queries(tq_olstar).mean():_.2f}\")\n",
    "print(f\"{total_queries(tq_lstar).mean():_.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short prefix rows\n",
    "\n",
    "The main benefit of using OL* is that it can decompose a full machine into smaller components, where each component models the behaviour for one output symbol. However, some models cannot be decomposed: the smallest component has as many states as the full machine. The number of states of components is reflected in the number of short prefix rows in the table during learning.\n",
    "\n",
    "In this section, we will analyse the two possible scenarios: the number of short prefix rows needed by OL* is smaller than the number of states of the SUL, and the number of short prefix rows needed by OL* is equal to the number of states of the SUL. Note that it is impossible for OL* to require more short prefix rows that there are states in the SUL, since we only add a short prefix row when there is an output for which this row differs from all previous rows, and the number of unique rows is precisely the number states of the SUL.\n",
    "\n",
    "#### Short prefix rows < Number of states\n",
    "\n",
    "We will start our analysis by looking at the scenario in which OL* needs fewer short prefix rows than the number of states in the SUL. Our hypothesis would be that OL* would then need fewer queries than L*, since the model has been effectively decomposed into smaller components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_189_797.15\n",
      "2_480_564.07\n"
     ]
    }
   ],
   "source": [
    "short_olstar = data_olstar[data_olstar[\"Short rows\"] < data_olstar[\"States\"]]\n",
    "short_lstar = data_lstar[data_olstar[\"Short rows\"] < data_olstar[\"States\"]]\n",
    "\n",
    "print(f\"{total_queries(short_olstar).mean():_.2f}\")\n",
    "print(f\"{total_queries(short_lstar).mean():_.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short prefix rows = Number of states\n",
    "\n",
    "When the number of short prefix rows needed by OL* is equal to the number of states of the SUL, we would expect OL* to either do similar to L*, since the algorithm reduces to doing L*, or to do worse, since it would require some extra queries to realise that the smallest component is the full SUL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_187_001.11\n",
      "1_652_586.41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model                 8.092308e+01\n",
       "Stages                1.175824e+00\n",
       "States                5.552198e+02\n",
       "Short rows            5.552198e+02\n",
       "Inconsistent count    4.703297e+00\n",
       "Zero outputs count    0.000000e+00\n",
       "Two outputs count     0.000000e+00\n",
       "Learning queries      4.322050e+05\n",
       "Learning symbols      4.492356e+06\n",
       "Testing queries       7.547961e+05\n",
       "Testing symbols       8.198559e+06\n",
       "dtype: float64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equal_olstar = data_olstar[data_olstar[\"Short rows\"] == data_olstar[\"States\"]]\n",
    "equal_lstar = data_lstar[data_olstar[\"Short rows\"] == data_olstar[\"States\"]]\n",
    "\n",
    "print(f\"{total_queries(equal_olstar).mean():_.2f}\")\n",
    "print(f\"{total_queries(equal_lstar).mean():_.2f}\")\n",
    "\n",
    "equal_olstar.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model               8.092308e+01\n",
       "Stages              1.307692e+00\n",
       "States              5.552198e+02\n",
       "Learning queries    3.301116e+05\n",
       "Learning symbols    2.969751e+06\n",
       "Testing queries     1.322475e+06\n",
       "Testing symbols     1.477462e+07\n",
       "dtype: float64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equal_lstar.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
